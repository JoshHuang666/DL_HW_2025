{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-21T09:08:27.603577Z",
     "iopub.status.busy": "2025-04-21T09:08:27.603079Z",
     "iopub.status.idle": "2025-04-21T09:08:27.611143Z",
     "shell.execute_reply": "2025-04-21T09:08:27.609738Z",
     "shell.execute_reply.started": "2025-04-21T09:08:27.603546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import zipfile  # ⭐️ 新增：壓縮功能\n",
    "import os       # ⭐️ 新增：檢查與建立檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T09:08:27.613779Z",
     "iopub.status.busy": "2025-04-21T09:08:27.613162Z",
     "iopub.status.idle": "2025-04-21T09:08:27.639345Z",
     "shell.execute_reply": "2025-04-21T09:08:27.638146Z",
     "shell.execute_reply.started": "2025-04-21T09:08:27.613740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Dataset ===\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word_vocab, pos_vocab, ner_vocab, max_len=128):\n",
    "        self.labels = labels\n",
    "        self.word_vocab = word_vocab\n",
    "        self.pos_vocab = pos_vocab\n",
    "        self.ner_vocab = ner_vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.tokenized_data = []\n",
    "\n",
    "        for text in texts:\n",
    "            doc = nlp(text)\n",
    "            tokens = [token.text.lower() for token in doc][:max_len]\n",
    "            pos_tags = [token.pos_ for token in doc][:max_len]\n",
    "            ner_tags = [token.ent_type_ if token.ent_type_ != \"\" else \"O\" for token in doc][:max_len]\n",
    "\n",
    "            # Padding\n",
    "            pad_len = max_len - len(tokens)\n",
    "            tokens += ['<pad>'] * pad_len\n",
    "            pos_tags += ['<pad>'] * pad_len\n",
    "            ner_tags += ['<pad>'] * pad_len\n",
    "\n",
    "            self.tokenized_data.append((tokens, pos_tags, ner_tags))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, pos_tags, ner_tags = self.tokenized_data[idx]\n",
    "        word_ids = [self.word_vocab.get(t, 1) for t in tokens]  # UNK fallback 是 1\n",
    "        pos_ids = [self.pos_vocab.get(p, 1) for p in pos_tags]\n",
    "        ner_ids = [self.ner_vocab.get(n, 1) for n in ner_tags]\n",
    "        return (torch.tensor(word_ids), torch.tensor(pos_ids), torch.tensor(ner_ids)), torch.tensor(self.labels[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T09:08:27.641644Z",
     "iopub.status.busy": "2025-04-21T09:08:27.641209Z",
     "iopub.status.idle": "2025-04-21T09:08:27.661374Z",
     "shell.execute_reply": "2025-04-21T09:08:27.660281Z",
     "shell.execute_reply.started": "2025-04-21T09:08:27.641609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# === 工具函數 ===\n",
    "def tokenize_with_features(text):\n",
    "    doc = nlp(text)\n",
    "    tokens, pos_tags, ner_tags = [], [], []\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            tokens.append(token.text.lower())\n",
    "            pos_tags.append(token.pos_)\n",
    "            ner_tags.append(token.ent_type_ if token.ent_type_ else \"O\")\n",
    "    return tokens, pos_tags, ner_tags\n",
    "\n",
    "def build_vocab_with_features(texts, min_freq=2):\n",
    "    word_counter, pos_set, ner_set = Counter(), set(), set()\n",
    "    for text in texts:\n",
    "        tokens, pos_tags, ner_tags = tokenize_with_features(text)\n",
    "        word_counter.update(tokens)\n",
    "        pos_set.update(pos_tags)\n",
    "        ner_set.update(ner_tags)\n",
    "    \n",
    "    # 建 word vocab，保留 PAD 和 UNK\n",
    "    word_vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for word, freq in word_counter.items():\n",
    "        if freq >= min_freq:\n",
    "            word_vocab[word] = len(word_vocab)\n",
    "    \n",
    "    # 建 pos vocab，加入 PAD 和 UNK\n",
    "    pos_vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for pos in sorted(pos_set):\n",
    "        pos_vocab[pos] = len(pos_vocab)\n",
    "    \n",
    "    # 建 ner vocab，加入 PAD 和 UNK\n",
    "    ner_vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for ner in sorted(ner_set):\n",
    "        ner_vocab[ner] = len(ner_vocab)\n",
    "    \n",
    "    return word_vocab, pos_vocab, ner_vocab\n",
    "\n",
    "\n",
    "\n",
    "def encode_text_with_features(text, word_vocab, pos_vocab, ner_vocab):\n",
    "    tokens, pos_tags, ner_tags = tokenize_with_features(text)\n",
    "    word_ids = [word_vocab.get(t, 1) for t in tokens]\n",
    "    pos_ids = [pos_vocab.get(p, 0) for p in pos_tags]\n",
    "    ner_ids = [ner_vocab.get(n, 0) for n in ner_tags]\n",
    "    \n",
    "    pad_len = MAX_LEN - len(word_ids)\n",
    "    if pad_len > 0:\n",
    "        word_ids += [0] * pad_len\n",
    "        pos_ids += [0] * pad_len\n",
    "        ner_ids += [0] * pad_len\n",
    "    else:\n",
    "        word_ids, pos_ids, ner_ids = word_ids[:MAX_LEN], pos_ids[:MAX_LEN], ner_ids[:MAX_LEN]\n",
    "    \n",
    "    return word_ids, pos_ids, ner_ids\n",
    "    \n",
    "def load_glove_embedding(path, vocab, dim):\n",
    "    embedding = np.random.randn(len(vocab), dim) * 0.05\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != dim + 1: continue\n",
    "            word = parts[0]\n",
    "            if word in vocab:\n",
    "                embedding[vocab[word]] = np.array(parts[1:], dtype=np.float32)\n",
    "    return torch.tensor(embedding, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T09:08:27.662853Z",
     "iopub.status.busy": "2025-04-21T09:08:27.662527Z",
     "iopub.status.idle": "2025-04-21T09:08:27.683457Z",
     "shell.execute_reply": "2025-04-21T09:08:27.682161Z",
     "shell.execute_reply.started": "2025-04-21T09:08:27.662830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Transformer Components ===\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -math.log(10000.0) / d_model)\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super().__init__()\n",
    "        assert d_model % nhead == 0, \"d_model 必須能被 nhead 整除\"\n",
    "        self.nhead = nhead\n",
    "        self.d_k = d_model // nhead\n",
    "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, self.nhead, self.d_k).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, T, C)\n",
    "        return self.out(x)\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, nhead)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2 = self.attn(x)\n",
    "        x = self.norm1(x + self.dropout(x2))\n",
    "        x2 = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(x2))\n",
    "        return x\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, pos_size, ner_size, d_model=128, nhead=4, d_ff=256,\n",
    "                 nlayers=2, num_classes=2, dropout=0.1, embedding_matrix=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_embed = nn.Embedding(pos_size, POS_EMBED_DIM, padding_idx=0)\n",
    "        self.ner_embed = nn.Embedding(ner_size, NER_EMBED_DIM, padding_idx=0)\n",
    "\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding.weight.data.copy_(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = True  # 可訓練\n",
    "\n",
    "        self.project = nn.Linear(EMBED_DIM + POS_EMBED_DIM + NER_EMBED_DIM, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, nhead, d_ff, dropout)\n",
    "            for _ in range(nlayers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        word_ids, pos_ids, ner_ids = inputs\n",
    "        word_emb = self.embedding(word_ids)\n",
    "        pos_emb = self.pos_embed(pos_ids)\n",
    "        ner_emb = self.ner_embed(ner_ids)\n",
    "        x = torch.cat([word_emb, pos_emb, ner_emb], dim=-1)\n",
    "        x = self.project(x)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.classifier(x[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T09:08:27.722142Z",
     "iopub.status.busy": "2025-04-21T09:08:27.721823Z",
     "iopub.status.idle": "2025-04-21T09:08:27.728500Z",
     "shell.execute_reply": "2025-04-21T09:08:27.727503Z",
     "shell.execute_reply.started": "2025-04-21T09:08:27.722118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct, count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for (word_ids, pos_ids, ner_ids), labels in dataloader:\n",
    "            word_ids, pos_ids, ner_ids, labels = word_ids.to(device), pos_ids.to(device), ner_ids.to(device), labels.to(device)\n",
    "            outputs = model((word_ids, pos_ids, ner_ids))\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            count += labels.size(0)\n",
    "    return correct / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T09:08:27.730535Z",
     "iopub.status.busy": "2025-04-21T09:08:27.730172Z",
     "iopub.status.idle": "2025-04-21T09:08:27.747243Z",
     "shell.execute_reply": "2025-04-21T09:08:27.746296Z",
     "shell.execute_reply.started": "2025-04-21T09:08:27.730481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def expand_embedding_dim(embedding_matrix, target_dim=256):\n",
    "    old_dim = embedding_matrix.shape[1]\n",
    "    if old_dim >= target_dim:\n",
    "        return embedding_matrix[:, :target_dim]\n",
    "    else:\n",
    "        pad = torch.randn(embedding_matrix.shape[0], target_dim - old_dim) * 0.05\n",
    "        return torch.cat([embedding_matrix, pad], dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T09:08:27.749630Z",
     "iopub.status.busy": "2025-04-21T09:08:27.749233Z",
     "iopub.status.idle": "2025-04-21T09:08:27.768623Z",
     "shell.execute_reply": "2025-04-21T09:08:27.767641Z",
     "shell.execute_reply.started": "2025-04-21T09:08:27.749598Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 訓練函數 ===\n",
    "def train_model(model, train_loader, val_loader, epochs, optimizer, criterion, device):\n",
    "    model.to(device)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, count = 0, 0, 0\n",
    "\n",
    "        for (word_ids, pos_ids, ner_ids), labels in train_loader:\n",
    "            word_ids, pos_ids, ner_ids, labels = word_ids.to(device), pos_ids.to(device), ner_ids.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model((word_ids, pos_ids, ner_ids))\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            count += labels.size(0)\n",
    "        \n",
    "        train_acc = correct / count\n",
    "        val_acc = evaluate(model, val_loader, device)\n",
    "        print(f\"[Epoch {epoch+1}] Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T09:08:27.770404Z",
     "iopub.status.busy": "2025-04-21T09:08:27.769749Z",
     "iopub.status.idle": "2025-04-21T09:08:28.604278Z",
     "shell.execute_reply": "2025-04-21T09:08:28.603037Z",
     "shell.execute_reply.started": "2025-04-21T09:08:27.770377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 參數 ===\n",
    "EMBED_DIM = 128\n",
    "N_HEAD = 4              # 注意力頭數\n",
    "BATCH_SIZE = 32\n",
    "DROPOUT = 0.1\n",
    "EPOCHS = 10\n",
    "D_FF = 512              # Feedforward 隱藏層大小\n",
    "N_LAYERS = 4             # Transformer layer 數量\n",
    "NUM_CLASSES = 15         # 分類類別數（根據資料集調整）\n",
    "MAX_LEN = 256           # 文字最大長度\n",
    "GLOVE_PATH = \"glove.6B.100d.txt\"  # 確保你有這個檔案\n",
    "POS_EMBED_DIM = 96\n",
    "NER_EMBED_DIM = 96\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T09:08:28.607259Z",
     "iopub.status.busy": "2025-04-21T09:08:28.606921Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Acc: 0.6620, Val Acc: 0.7412\n",
      "[Epoch 2] Train Acc: 0.7694, Val Acc: 0.7659\n",
      "[Epoch 3] Train Acc: 0.8001, Val Acc: 0.7563\n",
      "[Epoch 4] Train Acc: 0.8242, Val Acc: 0.7731\n",
      "[Epoch 5] Train Acc: 0.8471, Val Acc: 0.7748\n",
      "[Epoch 6] Train Acc: 0.8693, Val Acc: 0.7708\n",
      "[Epoch 7] Train Acc: 0.8882, Val Acc: 0.7643\n",
      "[Epoch 8] Train Acc: 0.9049, Val Acc: 0.7649\n",
      "[Epoch 9] Train Acc: 0.9162, Val Acc: 0.7611\n",
      "[Epoch 10] Train Acc: 0.9226, Val Acc: 0.7602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# === 主流程 ===\n",
    "\n",
    "def train_main():\n",
    "    df = pd.read_json(\"News_train.json\", lines=True)\n",
    "    texts = (df['headline'] + \" \" + df['short_description']).tolist()\n",
    "    labels = LabelEncoder().fit_transform(df['label'].tolist())\n",
    "    \n",
    "    word_vocab, pos_vocab, ner_vocab = build_vocab_with_features(texts)\n",
    "    embedding_matrix = load_glove_embedding(GLOVE_PATH, word_vocab, dim=100)\n",
    "    embedding_matrix = expand_embedding_dim(embedding_matrix, target_dim=EMBED_DIM)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "    train_ds = TextDataset(X_train, y_train, word_vocab, pos_vocab, ner_vocab)\n",
    "    val_ds = TextDataset(X_val, y_val, word_vocab, pos_vocab, ner_vocab)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = TransformerClassifier(\n",
    "        vocab_size=len(word_vocab),\n",
    "        pos_size=len(pos_vocab),\n",
    "        ner_size=len(ner_vocab),\n",
    "        d_model=EMBED_DIM,\n",
    "        nhead=N_HEAD,\n",
    "        d_ff=D_FF,\n",
    "        nlayers=N_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        embedding_matrix=embedding_matrix\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    train_model(model, train_loader, val_loader, EPOCHS, optimizer, criterion, device)\n",
    "    val_acc = evaluate(model, val_loader, device)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'word_vocab': word_vocab,\n",
    "        'pos_vocab': pos_vocab,\n",
    "        'ner_vocab': ner_vocab\n",
    "    }, 'model.pt')\n",
    "\n",
    "\n",
    "# 執行訓練\n",
    "train_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6554/2108126336.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"model.pt\", map_location=\"cpu\")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Inference ---\n",
    "def inference_main():\n",
    "    df = pd.read_json(\"News_test.json\", lines=True)\n",
    "    texts = (df['headline'] + \" \" + df['short_description']).tolist()\n",
    "\n",
    "    checkpoint = torch.load(\"model.pt\", map_location=\"cpu\")\n",
    "    word_vocab, pos_vocab, ner_vocab = checkpoint['word_vocab'], checkpoint['pos_vocab'], checkpoint['ner_vocab']\n",
    "\n",
    "    embedding_matrix = load_glove_embedding(GLOVE_PATH, word_vocab, dim=100)\n",
    "    embedding_matrix = expand_embedding_dim(embedding_matrix, target_dim=EMBED_DIM)\n",
    "\n",
    "    model = TransformerClassifier(\n",
    "    vocab_size=len(word_vocab),\n",
    "    pos_size=len(pos_vocab),\n",
    "    ner_size=len(ner_vocab),\n",
    "    d_model=EMBED_DIM,\n",
    "    nhead=N_HEAD,\n",
    "    d_ff=D_FF,\n",
    "    nlayers=N_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    embedding_matrix=embedding_matrix\n",
    ")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    test_ds = TextDataset(texts, [0]*len(texts), word_vocab, pos_vocab, ner_vocab)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "    preds = []\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in test_loader:\n",
    "            out = model(x)\n",
    "            pred = out.argmax(dim=1).cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "\n",
    "    # 生成 submission.csv\n",
    "    pd.DataFrame({\"ID\": list(range(len(preds))), \"label\": preds}).to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "# 執行推論\n",
    "inference_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer: 使用 spaCy 的 en_core_web_sm 模型\n",
    "理由：spaCy 是一個強大的 NLP 工具包，提供高品質的 tokenization、POS 標註、NER 辨識等功能。選擇 en_core_web_sm 是因為它是輕量、適用於英文的模型，效能與準確率在小型任務中表現良好。\n",
    "\n",
    "僅保留 英文單詞（is_alpha），過濾數字、標點等無意義 token。\n",
    "\n",
    "對每個 token 擷取：\n",
    "token.text.lower()：小寫化單詞（標準化）\n",
    "token.pos_：詞性（POS）標註\n",
    "token.ent_type_：命名實體（NER）標註，若無則標為 \"O\"（代表非實體）\n",
    "\n",
    "特徵編碼與填充：\n",
    "\n",
    "每筆文字會被轉換為\n",
    "word_ids, pos_ids, ner_ids → 對應 vocab 索引，支援 <UNK> (1), <PAD> (0)\n",
    "長度不足則用 0 補齊（padding），超過最大長度 MAX_LEN=256 則截斷\n",
    "\n",
    "詞彙表建構：\n",
    "word_vocab 根據詞頻（最小出現次數 min_freq=2）建立，避免雜訊\n",
    "pos_vocab 與 ner_vocab 則為集合，不考慮頻率（因為類別較少）\n",
    "\n",
    "Embedding：\n",
    "Word embedding 可載入 GloVe (load_glove_embedding)，維度由外部參數指定（如 100d）(kaggle 公開dataset )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Structure 與 Hyperparameters 設計說明  \n",
    "\n",
    "主架構：基於 Transformer Encoder 的分類模型\n",
    "\n",
    "<pre>\n",
    "超參數                  值                                    說明與理由  \n",
    "\n",
    "EMBED_DIM              256\t                      word embedding 維度（若用 GloVe 可為 100，這裡為內部投影後總維度）  \n",
    "\n",
    "POS_EMBED_DIM\t        16\t                      POS 類別較少，維度不需太高；足以捕捉語法資訊  \n",
    "\n",
    "NER_EMBED_DIM\t        16\t                      同 POS。加入實體資訊有助於語義理解  \n",
    "\n",
    "d_model\t               256\t                      Transformer 的內部維度，也是每個 token 的最終表示向量大小  \n",
    "\n",
    "nhead\t                4\t                      將 d_model=256 分成 4 頭，每頭 64 維，利於模型捕捉多樣化注意力模式  \n",
    "\n",
    "d_ff\t               512\t                      Feedforward 隱藏層大小，設為 2×d_model，常見設計  \n",
    "\n",
    "nlayers\t                4\t                      疊加 4 層 Encoder；足以學習中階語義特徵，平衡效能與計算成本  \n",
    "\n",
    "dropout\t               0.1\t                      防止過擬合，常見的 Transformer 預設值  \n",
    "\n",
    "MAX_LEN\t               256\t                      限定句子長度，兼顧記憶體與資訊保留，對大部分文檔已足夠  \n",
    "\n",
    "classifier\t      Linear(d_model → num_classes)   使用 [CLS] token 對整句分類\n",
    "<pre>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 715814,
     "sourceId": 1246668,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7148591,
     "sourceId": 11413816,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7148932,
     "sourceId": 11414467,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
